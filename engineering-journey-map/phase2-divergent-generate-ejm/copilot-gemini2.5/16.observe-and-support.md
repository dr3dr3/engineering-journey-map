# 16. Observe and Support

Responding to production incidents, troubleshooting issues, and coordinating resolution efforts.

## Journey Step Focus

*   How do we respond to production incidents quickly and effectively?
*   How do we coordinate the incident response effort?
*   How do we troubleshoot and diagnose the root cause of an incident?
*   How do we communicate the status of an incident to stakeholders?
*   How do we learn from every incident to prevent it from happening again?
*   How do we manage the stress and pressure of being on-call?

## Actions

*   Acknowledging an alert and taking ownership of the incident.
*   Joining a dedicated "war room" (a video conference or chat channel) to coordinate the response.
*   Following a runbook to perform initial diagnosis and mitigation steps.
*   Analyzing logs, metrics, and traces to identify the root cause of the issue.
*   Escalating the incident to other teams or subject matter experts if necessary.
*   Implementing a fix or a workaround to restore service.
*   Communicating regular updates to stakeholders via a status page or other channel.
*   Conducting a "blameless post-mortem" after the incident is resolved to identify the root cause and contributing factors, and to define follow-up actions.

## Challenges

*   The on-call engineer is paged for a non-actionable alert.
*   It's difficult to get the right people involved in the incident response.
*   The root cause of the issue is not immediately obvious.
*   The pressure is high to restore service as quickly as possible, which can lead to mistakes.
*   Communication with stakeholders is chaotic and inconsistent.
*   The same incidents keep happening over and over again.

## Interactions

*   **On-call Engineer:** The first responder to a production incident.
*   **Incident Commander:** The person who is responsible for coordinating the overall incident response.
*   **Subject Matter Experts:** Engineers from other teams who are pulled in to help with a specific part of the system.
*   **IT Operations/NOC:** Often the first to notice an issue and can help with initial triage.
*   **Customer Support:** The voice of the customer, providing information on how the incident is impacting users.
*   **Communications Lead:** Responsible for communicating with external stakeholders, such as customers.

## Touchpoints

*   **Alerting Channel (e.g., PagerDuty, Opsgenie):** Where the on-call engineer is notified of an incident.
*   **Incident Response "War Room" (e.g., Zoom, Slack):** The central hub for coordinating the response.
*   **Monitoring and Observability Platform (e.g., Datadog, Splunk):** The primary source of data for troubleshooting.
*   **Runbooks:** Documents that provide step-by-step instructions for handling specific incidents.
*   **Status Page:** A page for communicating the status of an incident to stakeholders.
*   **Post-mortem Template:** A template for documenting the details of an incident and the follow-up actions.
*   **Incident Management Tool (e.g., incident.io):** A tool for managing the lifecycle of an incident.

## Feeling

*   üò• **Stressed:** The pressure is on to fix the problem as quickly as possible.
*   ü§î **Focused:** Deep in concentration, trying to solve a complex puzzle.
*   ü§ù **Collaborative:** Working closely with a team of people to resolve the incident.
*   üòå **Relieved:** When the incident is resolved and the service is restored.
*   ü§ì **Curious:** Eager to understand the root cause of the incident and how to prevent it in the future.

## Opportunities

*   Establish a clear, well-defined incident response process that is regularly practiced.
*   Invest in building high-quality runbooks that are easy to follow.
*   Promote a "blameless" culture where the focus is on learning, not blaming.
*   Automate as much of the incident response process as possible.
*   Provide training on incident management and on-call best practices.
*   Create a sustainable on-call rotation that minimizes burnout.

## Potential for AI

*   **AI-Powered Incident Triage:** An AI assistant that can automatically categorize and prioritize incidents, and can suggest the right runbook to use.
*   **Automated Root Cause Analysis:** An AI system that can analyze all the data from an incident and identify the most likely root cause.
*   **Intelligent Runbook Execution:** An AI assistant that can automatically execute some of the steps in a runbook, such as gathering more data or restarting a service.
*   **Automated Post-mortem Generation:** An AI tool that can automatically generate a draft post-mortem report by summarizing the timeline, key events, and data from an incident.
*   **On-call Support Bot:** An AI-powered chatbot that can answer questions from the on-call engineer, provide context about the system, and help them to find the right information.
